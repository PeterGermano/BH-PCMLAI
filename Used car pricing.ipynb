{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from kaggle that contains information on 3 million used cars.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "Rudy Russo has a problem. We've seen how bad business is. Thanks to Fuchs, his name is mud.\n",
    "\n",
    "Something that may help Rudy is to accurately price used cars as appropriately priced cars tend to sell faster.\n",
    "\n",
    "By leveraging existing data, we can build a supervised learning model using the sale price for predictions.\n",
    "    The analysis will highlight factors influincing the price \n",
    "    The model will then identify the correlations that impact pricing\n",
    "    A validated model can then be used to provide informed pricing on prospective vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tools for programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_log_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy = pd.read_csv('data/vehicles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List columns, counts and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize data and headers within the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a random sample of the table to glimpse at data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the columns to look at the counts, averages and ranges of data within columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the id and vin columns. They contain unique identifiers which are not needed for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2 = rudy.drop(columns = ['id','VIN'], axis=1)\n",
    "rudy2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrub the data to contain only lowercase and remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in rudy2.columns[1:]:\n",
    "    if rudy2[column].dtype == 'object':\n",
    "        rudy2[column] = rudy2[column].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify null values with columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill null values with values derived from the column (impute). Two methods will be used to impute values, Extra trees regression for numarical data and Bayesian Ridge for catigorical data. To apply these methods, the data will be split by data type. Float columns are numerical and go the th eExtratreesregressor while object categories are categorical and go to the Baysesian Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers = [\n",
    "    BayesianRidge(),\n",
    "    ExtraTreesRegressor(n_estimators=10, random_state=0),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify numerical and categorical columns, impute values and check for remaining null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floats = ['year', 'odometer']\n",
    "\n",
    "object = list((Counter(rudy2.columns) -\\\n",
    "                    Counter(floats + ['manufacturer', 'model', 'price'])).elements())\n",
    "sr_floats = rudy2[floats]\n",
    "imp_floats = IterativeImputer(imputers[1])\n",
    "imputed_vals = imp_floats.fit_transform(sr_floats)\n",
    "rudy2[floats] = imputed_vals\n",
    "rudy2.isnull().sum()[floats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data_col):\n",
    "    vals = np.array(data_col.dropna())\n",
    "    reshaped_data = vals.reshape(-1,1)\n",
    "    encoded_data = encoder.fit_transform(reshaped_data)\n",
    "    data_col.loc[data_col.notnull()] = np.squeeze(encoded_data)\n",
    "    return data_col\n",
    "\n",
    "sr_object = rudy2[object]\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "for column in object:\n",
    "    encode(sr_object[column])\n",
    "    imp_categorical = IterativeImputer(BayesianRidge())\n",
    "    imputed_vals_cat = imp_categorical.fit_transform(sr_object[column].values.reshape(-1, 1))\n",
    "    imputed_vals_cat = imputed_vals_cat.astype('int64')\n",
    "    imputed_vals_cat = pd.DataFrame(imputed_vals_cat)\n",
    "    imputed_vals_cat = encoder.inverse_transform(imputed_vals_cat.values.reshape(-1, 1))\n",
    "    sr_object[column] = imputed_vals_cat\n",
    "\n",
    "rudy2[object]= sr_object\n",
    "rudy2.isnull().sum()[object]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the table header and random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the price will be the main focus of the analysis, a glimpse of the top to prices will be helpful in understanding the distribution of prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_counts = rudy2.price.value_counts()\n",
    "print('Price: ', price_counts.index[0], '\\nCounts: ', price_counts.values[0])\n",
    "print('\\nTen most frequently occurring prices:\\n')\n",
    "print(price_counts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram will help visualize the price column and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(color_codes=True)\n",
    "sns.set(rc={'figure.figsize':(6,3)})\n",
    "\n",
    "def plot_histogram(col, color_val='#005c9d',\\\n",
    "                   x_label='Price [x10\\u2076 USD]', y_label='Frequency',\\\n",
    "                   title_text='Distribution of car prices'):\n",
    "    sns.distplot(col, kde=False, color=color_val)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title_text)\n",
    "    ax.get_xaxis().get_major_formatter().set_scientific(False)\n",
    "    ax.get_yaxis().get_major_formatter().set_scientific(False)\n",
    "\n",
    "    plt.show()\n",
    "price_mill = rudy2.price/10**6\n",
    "plot_histogram(price_mill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some prices seem to be outlyers to the range. An histogram limiting the range to less than $75000 provides a better view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(rudy2.price[rudy2.price<75000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the price to a log of the price will shift the distibution more toward a normal distibution and possibly provide a better fit for the model. The log price is generated and added as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2.insert(1, 'logprice', np.log1p(rudy2['price']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log of the Price plotted in a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(rudy2.logprice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price column is limited to the difference between the upper and lower quartile. This is known as the interquartile range (IQR). Using the IQR, outliers at the low and high ranges of the dataset are removed, allowing for analysis to be based on the middle half of the distribution and less influenced by extreme values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2['price'] = rudy2.price.astype(str)\n",
    "Q1 = rudy2.quantile(0.25)\n",
    "Q3 = rudy2.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "rudy2 = rudy2[~((rudy2 < (Q1 - 1.5 * IQR)) | (rudy2 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "rudy2['price'] = rudy2.price.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and manufacturer columns are cleaned up by dropping model names that appear less than 1000 times.  The model data is then used to fill in manufacturer null values by providing the most common manufacturer for that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2 = rudy2.groupby(\"model\").filter(lambda x: len(x) >= 1000)\n",
    "rudy2.reset_index(drop=True, inplace=True)\n",
    "rudy2['manufacturer'] = rudy2.groupby('model').manufacturer.transform(\n",
    "    lambda x: x.fillna(x.mode()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manufacturers are then visualized in a count plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.xticks(rotation=90)\n",
    "sns.countplot(rudy2.manufacturer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between numerical columns are explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a positive correlation between price and year, implying that as year increases, the price increases. This makes sense since newer cars typically cost more and observed in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = rudy2.year.astype(np.int64)\n",
    "price = rudy2.price\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(year, price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the distribution of categorical data through count plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obplot = object.copy()\n",
    "obplot.remove('region')\n",
    "obplot.remove('state')\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(20, 15))\n",
    "for variable, subplot in zip(obplot, ax.flatten()):\n",
    "    sns.countplot(rudy2[variable], ax=subplot)\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how the categorical data relates to price when plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(20, 15))\n",
    "for var, subplot in zip(obplot, ax.flatten()):\n",
    "    sns.barplot(x=var, y='price', data=rudy2, ax=subplot)\n",
    "    for label in subplot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we proceed toward the modelling of the data, the \"price\" column can be dropped since the log price will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2.drop('price', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['logprice']\n",
    "rearranged_cols = np.hstack((rudy2.columns.difference(col_list, sort=False), col_list))\n",
    "\n",
    "rudy2 = rudy2.reindex(columns=rearranged_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and categorical columns are listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object = list((Counter(rudy2.columns) -\\\n",
    "                    Counter(floats + ['logprice', 'price'])).elements())\n",
    "object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2[object] = rudy2[object].apply(encoder.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the dataframe is checked again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical data is transformed into numerical data to be used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = floats + ['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "for col in columns_to_scale:\n",
    "   rudy2[col] = scaler.fit_transform(np.array(rudy2[col]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IQR of the log price is updated in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = rudy2.logprice.quantile(0.25)\n",
    "Q3 = rudy2.logprice.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "rudy3 = rudy2[(rudy2.logprice >= (Q1 - 1.5 * IQR)) & (rudy2.logprice <= (Q3 + 1.5 * IQR))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy3.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at the plot of year and log price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy_plot1 = sns.scatterplot(data=rudy3, x=\"logprice\", y=\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a look at the scatter plot of the odometer and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rudy_plot2 = sns.scatterplot(data=rudy3, x=\"logprice\", y=\"odometer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then split inot training and test sets in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rudy3.drop(columns = 'logprice', axis = 1)\n",
    "y = rudy3['logprice']\n",
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to remove negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_negatives(y_test, y_pred):\n",
    "    ind = [index for index in range(len(y_pred)) if(y_pred[index]>0)]\n",
    "    y_pred = y_pred[ind]\n",
    "    y_test = y_test[ind]\n",
    "    return (y_test, y_pred)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a function to measure the metrics of the model are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_perf(y_test, y_pred):\n",
    "    res = []\n",
    "    res.append(mean_squared_log_error(y_test, y_pred))\n",
    "    res.append(np.sqrt(res[0]))\n",
    "    res.append(r2_score(y_test, y_pred))\n",
    "    res.append(round(r2_score(y_test, y_pred)*100, 4))\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rudy_metrics = pd.DataFrame(index=['MSLE', 'RMSLE','R2 score','Accuracy(%)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline look at the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_train = np.ones(shape = y_train.shape)*y_train.mean()\n",
    "baseline_test = np.ones(shape = y_test.shape)*y_test.mean()\n",
    "mse_baseline_train = mean_squared_error(baseline_train, y_train)\n",
    "mse_baseline_test = mean_squared_error(baseline_test, y_test)\n",
    "print(baseline_train.shape, baseline_test.shape)\n",
    "print(f'Baseline for training data: {mse_baseline_train}')\n",
    "print(f'Baseline for testing data: {mse_baseline_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is fit to a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_pred = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a check of the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_1, y_pred_1 = remove_negatives(y_test, y_pred)\n",
    "res_lin_reg = evaluate_perf(y_test_1, y_pred_1)\n",
    "\n",
    "print(\"Coefficients: \\n\", lin_reg.coef_)\n",
    "print(f\"MSLE : {res_lin_reg[0]}\")\n",
    "print(f\"Root MSLE : {res_lin_reg[1]}\")\n",
    "print(f\"R2 Score : {res_lin_reg[2]} or {res_lin_reg[3]}%\")\n",
    "\n",
    "Rudy_metrics[\"Linear\"] = res_lin_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the actual ve predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lin_comp = pd.DataFrame({'Actual': y_test_1, 'Predicted': y_pred_1})\n",
    "df_lin_comp = df_lin_comp.head(25)\n",
    "\n",
    "df_lin_comp.plot(kind='bar', figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.1', color='green')\n",
    "plt.title('Linear regressor: Actual vs. predicted')\n",
    "plt.ylabel('MSLE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a look at the coefficients ranked by importance.  Overall, the linear regression model does not look very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(lin_reg.coef_, index = X_train.columns)\n",
    "sorted_coefs = coefs.sort_values()\n",
    "\n",
    "sorted_coefs.plot(kind = \"barh\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "plt.xlabel('Score'); \n",
    "plt.ylabel('Feature'); \n",
    "plt.title('Linear regressor: Feature importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model is the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rando_reg = RandomForestRegressor()\n",
    "n_estimators = [25, 50, 75, 100],\n",
    "max_features = 0.5,\n",
    "max_depth = [5, 10, 20],\n",
    "min_samples_split = [1, 5, 10, 50, 100],\n",
    "min_samples_leaf = [1, 5, 10, 50, 100],\n",
    "bootstrap = [True, False]\n",
    "rando_reg.fit(X_train,y_train)\n",
    "y_pred2 = rando_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_2, y_pred_2 = remove_negatives(y_test, y_pred)\n",
    "res_rf_reg = evaluate_perf(y_test_2, y_pred_2)\n",
    "\n",
    "print(f\"MSLE : {res_rf_reg[0]}\")\n",
    "print(f\"Root MSLE : {res_rf_reg[1]}\")\n",
    "print(f\"R2 Score : {res_rf_reg[2]} or {res_rf_reg[3]}%\")\n",
    "\n",
    "Rudy_metrics['RandomForest'] = res_rf_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_comp = pd.DataFrame({'Actual': y_test_2, 'Predicted': y_pred_2})\n",
    "df_rf_comp = df_rf_comp.head(25)\n",
    "\n",
    "df_rf_comp.plot(kind='bar', figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.1', color='green')\n",
    "plt.title('Random forest: Actual vs. predicted')\n",
    "plt.ylabel('MSLE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_param_dict = {'ridge__alpha': np.logspace(0, 10, 50)}\n",
    "ridge_pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                      ('ridge', Ridge())])\n",
    "ridge_grid = GridSearchCV(ridge_pipe, param_grid=ridge_param_dict)\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "ridge_train_preds = ridge_grid.predict(X_train)\n",
    "ridge_test_preds = ridge_grid.predict(X_test)\n",
    "ridge_train_mse = mean_squared_error(y_train, ridge_train_preds)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_test_preds)\n",
    "print(f'Train MSE: {ridge_train_mse}')\n",
    "print(f'Test MSE: {ridge_test_mse}')\n",
    "ridge_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipe.fit(X_train,y_train)\n",
    "y_pred3 = ridge_pipe.predict(X_test)\n",
    "y_test_3, y_pred_3 = remove_negatives(y_test, y_pred)\n",
    "res_ridge_reg = evaluate_perf(y_test_3, y_pred_3)\n",
    "\n",
    "print(f\"MSLE : {res_ridge_reg[0]}\")\n",
    "print(f\"Root MSLE : {res_ridge_reg[1]}\")\n",
    "print(f\"R2 Score : {res_ridge_reg[2]} or {res_ridge_reg[3]}%\")\n",
    "\n",
    "Rudy_metrics['Ridge'] = res_ridge_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ridge_comp = pd.DataFrame({'Actual': y_test_3, 'Predicted': y_pred_3})\n",
    "df_ridge_comp = df_ridge_comp.head(25)\n",
    "\n",
    "df_ridge_comp.plot(kind='bar', figsize=(10,5))\n",
    "\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.1', color='green')\n",
    "plt.title('Ridge: Actual vs. predicted')\n",
    "plt.ylabel('MSLE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight on drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine tuning their inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
